{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c4b72b",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding (BPE) \n",
    "\n",
    "was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a20cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a92e0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/home/silver/NLP_Basics/Tokenazation/phx.txt'\n",
    "\n",
    "model = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d5fa771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --model_type=bpe --input=/home/silver/NLP_Basics/Tokenazation/phx.txt --model_prefix=models/bpe --vocab_size=500\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "  input_format: \n",
      "  model_prefix: models/bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9349 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1493352\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9587% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999588\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 9349 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9349\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 27121\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=33814 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8127 size=20 all=1858 active=1681 piece=▁'\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5204 size=40 all=2596 active=2419 piece=ry\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3627 size=60 all=3157 active=2980 piece=..\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2654 size=80 all=3852 active=3675 piece=▁wh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1976 size=100 all=4436 active=4259 piece=al\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1920 min_freq=117\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1658 size=120 all=5086 active=1556 piece=ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1371 size=140 all=5771 active=2241 piece=▁A\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1186 size=160 all=6180 active=2650 piece=ce\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=971 size=180 all=6731 active=3201 piece=▁fe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=841 size=200 all=7067 active=3537 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=840 min_freq=113\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=764 size=220 all=7737 active=1617 piece=art\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=683 size=240 all=8064 active=1944 piece=▁al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=611 size=260 all=8487 active=2367 piece=ss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=583 size=280 all=8703 active=2583 piece=▁any\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=544 size=300 all=9121 active=3001 piece=▁-'\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=542 min_freq=98\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=500 size=320 all=9410 active=1290 piece=▁got\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=466 size=340 all=9617 active=1497 piece=▁tim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=428 size=360 all=9989 active=1869 piece=▁looking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=393 size=380 all=10290 active=2170 piece=ond\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=365 size=400 all=10541 active=2421 piece=ang\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=364 min_freq=85\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=340 size=420 all=10923 active=1345 piece=pt\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    \"--model_type=bpe \"\n",
    "    f\"--input={data} \"\n",
    "    f\"--model_prefix={model}/bpe \"\n",
    "    \"--vocab_size=500\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c6ae3",
   "metadata": {},
   "source": [
    "- Loaded all 9349 sentences.\n",
    "- It means that he read more than 1,400,000 characters, and the character coverage is close to 100% (not even a single strange character was missed).\n",
    "- The model was successfully trained without any problems. Use your file and cut the text into small pieces (BPE pieces) of 9349 lines.\n",
    "   You now have two files ready:\n",
    "\n",
    "   models/bpe.model: Contains the trained model.\n",
    "\n",
    "   models/bpe.vocab: Contains the list of symbols (vocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b3c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"File exists?\", os.path.exists(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8261cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model):\n",
    "    os.makedirs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "facb4cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(model, 'bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e57b571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"It's a Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', 't', \"'\", 's', '▁a', '▁T', 'est']\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(input_string))  #['▁I', 't', \"'\", 's', '▁a', '▁T', 'est']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb293d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 439, 454, 445, 5, 84, 355]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_ids(input_string))   #[81, 439, 454, 445, 5, 84, 355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d611d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Test\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁I', 't', \"'\", 's', '▁a', '▁T', 'est']))  #It's a Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9ad94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Test\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode_ids([81, 439, 454, 445, 5, 84, 355]))  #It's a Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51fc93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 500\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(f\"vocab size: {sp.get_piece_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8252ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 101 to piece: ur\n",
      "Piece ▁is to id: 225\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "print(f\"id 101 to piece: {sp.id_to_piece(101)}\")\n",
    "print(f\"Piece ▁is to id: {sp.piece_to_id('▁is')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56baab",
   "metadata": {},
   "source": [
    "id_to_piece() returns the token (subword) that represents the number. Conversely, piece_to_id() returns the token number.\n",
    "\n",
    "What's important?\n",
    "What distinguishes SentencePiece from many other libraries is that:\n",
    "\n",
    "It converts your sentence to IDs or tokens in a way that doesn't change the meaning.\n",
    "\n",
    "More importantly, you can return the sentence as it is from the tokens or IDs without losing spaces, upper/lowercase letters, punctuation, etc.\n",
    "\n",
    "In other traditional methods, the tokenizer breaks the sentence down for you without losing any information (for example, where the space is or how the punctuation is stored).\n",
    "\n",
    "With SentencePiece, the subwords contain information about the space (the \" \" represents a space), so when you decode the tokens, the sentence is returned exactly as you entered it.\n",
    "\n",
    "✅ In short, this is a huge advantage of SentencePiece over many other tokenization technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f55d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['▁I', 't', \"'\", 's', '▁a', '▁T', 'est']\n",
    "merged = \"\".join(tokens).replace('▁', \" \").strip()\n",
    "assert merged == input_string, \"Input string and detokenized sentence didn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eafac5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72772914",
   "metadata": {},
   "source": [
    "This means that the first 3 IDs are always system-specific symbols and not actual text words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23359abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command:     --model_type=bpe    --input=/home/silver/NLP_Basics/Tokenazation/phx.txt    --model_prefix=models/bpe_user    --user_defined_symbols=<sep>,<cls>    --vocab_size=500\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "  input_format: \n",
      "  model_prefix: models/bpe_user\n",
      "  model_type: BPE\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9349 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1493352\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9587% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999588\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 9349 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9349\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 27121\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=33814 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8127 size=20 all=1858 active=1681 piece=▁'\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5204 size=40 all=2596 active=2419 piece=ry\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3627 size=60 all=3157 active=2980 piece=..\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2654 size=80 all=3852 active=3675 piece=▁wh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1976 size=100 all=4436 active=4259 piece=al\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1920 min_freq=117\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1658 size=120 all=5086 active=1556 piece=ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1371 size=140 all=5771 active=2241 piece=▁A\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1186 size=160 all=6180 active=2650 piece=ce\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=971 size=180 all=6731 active=3201 piece=▁fe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=841 size=200 all=7067 active=3537 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=840 min_freq=113\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=764 size=220 all=7737 active=1617 piece=art\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=683 size=240 all=8064 active=1944 piece=▁al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=611 size=260 all=8487 active=2367 piece=ss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=583 size=280 all=8703 active=2583 piece=▁any\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=544 size=300 all=9121 active=3001 piece=▁-'\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=542 min_freq=98\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=500 size=320 all=9410 active=1290 piece=▁got\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=466 size=340 all=9617 active=1497 piece=▁tim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=428 size=360 all=9989 active=1869 piece=▁looking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=393 size=380 all=10290 active=2170 piece=ond\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=365 size=400 all=10541 active=2421 piece=ang\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=364 min_freq=85\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=340 size=420 all=10923 active=1345 piece=pt\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/bpe_user.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/bpe_user.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Example of user defined symbols\n",
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=bpe\\\n",
    "    --input={data}\\\n",
    "    --model_prefix={model}/bpe_user\\\n",
    "    --user_defined_symbols=<sep>,<cls>\\\n",
    "    --vocab_size=500''')\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load(os.path.join(model, 'bpe_user.model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bba1ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁wor', 'ld', '<cls>']\n"
     ]
    }
   ],
   "source": [
    "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04761562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sp_user.piece_to_id('<sep>')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93f50354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(sp_user.piece_to_id('<cls>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "879570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3= <sep>\n"
     ]
    }
   ],
   "source": [
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36c0a91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4= <cls>\n"
     ]
    }
   ],
   "source": [
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc7d6fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos= 1\n",
      "eos= 2\n",
      "unk= 0\n",
      "pad= -1\n"
     ]
    }
   ],
   "source": [
    "print('bos=', sp_user.bos_id())     # 1\n",
    "print('eos=', sp_user.eos_id())     # 2\n",
    "print('unk=', sp_user.unk_id())     # 0\n",
    "print('pad=', sp_user.pad_id())     # -1, disabled by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22212952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 148, 441, 340, 79]\n",
      "[1, 28, 148, 441, 340, 79, 2]\n"
     ]
    }
   ],
   "source": [
    "print(sp_user.encode_as_ids('Hello world'))    \n",
    "\n",
    "# Prepend or append bos/eos ids.\n",
    "print([sp_user.bos_id()] + sp_user.encode_as_ids('Hello world') + [sp_user.eos_id()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071de63e",
   "metadata": {},
   "source": [
    "# Explanation of BPE Tokenization Process\n",
    "\n",
    "## What is BPE Tokenization?\n",
    "\n",
    "BPE (Byte Pair Encoding) is a subword tokenization method that:\n",
    "\n",
    "- Splits text into smaller subword units rather than full words or characters.\n",
    "- Reduces vocabulary size efficiently.\n",
    "- Handles rare or unseen words by breaking them into known subword pieces.\n",
    "- Works well with complex languages and new word forms.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps We Took in BPE Tokenization\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   - Used a text file (e.g., `phx.txt`) containing training corpus for tokenization.\n",
    "\n",
    "2. **Training SentencePiece BPE Model**  \n",
    "   - Called SentencePiece trainer with parameters:\n",
    "     - `model_type=bpe`\n",
    "     - `vocab_size=500`\n",
    "     - Input file path and model prefix to save output.\n",
    "   - The trainer learns frequent byte-pairs and merges them iteratively.\n",
    "\n",
    "3. **Training Process**  \n",
    "   - Counts characters and byte pairs frequency.\n",
    "   - Merges most frequent pairs to form new tokens.\n",
    "   - Builds a vocabulary of subword tokens + special tokens like `<unk>`, `<s>`, `</s>`.\n",
    "\n",
    "4. **Output Files**  \n",
    "   - `bpe.model`: The trained tokenizer model.\n",
    "   - `bpe.vocab`: The vocabulary list with tokens and their frequencies.\n",
    "\n",
    "5. **Encoding & Decoding**  \n",
    "   - Encode a sentence to a list of subword tokens or integer IDs.\n",
    "   - Decode tokens or IDs back to original text perfectly (including spaces).\n",
    "   - Example:\n",
    "     ```python\n",
    "     input_string = \"This is a test\"\n",
    "     tokens = ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
    "     ```\n",
    "     The special symbol `▁` marks a space before the token.\n",
    "\n",
    "6. **Special/User-defined Tokens**  \n",
    "   - Added tokens like `<sep>`, `<cls>` as single indivisible tokens.\n",
    "   - Useful for models like BERT that need special markers.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use BPE Tokenization?\n",
    "\n",
    "- Balances between word-level and character-level tokenization.\n",
    "- Supports an open vocabulary that can handle unknown words.\n",
    "- Ensures reversible tokenization, so original text can be exactly recovered.\n",
    "- Allows use of special tokens to guide deep learning models.\n",
    "\n",
    "---# Explanation of BPE Tokenization Process\n",
    "\n",
    "## What is BPE Tokenization?\n",
    "\n",
    "BPE (Byte Pair Encoding) is a subword tokenization method that:\n",
    "\n",
    "- Splits text into smaller subword units rather than full words or characters.\n",
    "- Reduces vocabulary size efficiently.\n",
    "- Handles rare or unseen words by breaking them into known subword pieces.\n",
    "- Works well with complex languages and new word forms.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps We Took in BPE Tokenization\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   - Used a text file (e.g., `phx.txt`) containing training corpus for tokenization.\n",
    "\n",
    "2. **Training SentencePiece BPE Model**  \n",
    "   - Called SentencePiece trainer with parameters:\n",
    "     - `model_type=bpe`\n",
    "     - `vocab_size=500`\n",
    "     - Input file path and model prefix to save output.\n",
    "   - The trainer learns frequent byte-pairs and merges them iteratively.\n",
    "\n",
    "3. **Training Process**  \n",
    "   - Counts characters and byte pairs frequency.\n",
    "   - Merges most frequent pairs to form new tokens.\n",
    "   - Builds a vocabulary of subword tokens + special tokens like `<unk>`, `<s>`, `</s>`.\n",
    "\n",
    "4. **Output Files**  \n",
    "   - `bpe.model`: The trained tokenizer model.\n",
    "   - `bpe.vocab`: The vocabulary list with tokens and their frequencies.\n",
    "\n",
    "5. **Encoding & Decoding**  \n",
    "   - Encode a sentence to a list of subword tokens or integer IDs.\n",
    "   - Decode tokens or IDs back to original text perfectly (including spaces).\n",
    "   - Example:\n",
    "     ```python\n",
    "     input_string = \"This is a test\"\n",
    "     tokens = ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
    "     ```\n",
    "     The special symbol `▁` marks a space before the token.\n",
    "\n",
    "6. **Special/User-defined Tokens**  \n",
    "   - Added tokens like `<sep>`, `<cls>` as single indivisible tokens.\n",
    "   - Useful for models like BERT that need special markers.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use BPE Tokenization?\n",
    "\n",
    "- Balances between word-level and character-level tokenization.\n",
    "- Supports an open vocabulary that can handle unknown words.\n",
    "- Ensures reversible tokenization, so original text can be exactly recovered.\n",
    "- Allows use of special tokens to guide deep learning models.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

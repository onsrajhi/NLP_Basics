{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c225dd8",
   "metadata": {},
   "source": [
    "Tokenization using spacy\n",
    "\n",
    "When working with natural language processing (NLP), understanding how text is broken down is fundamental. At the heart of this process is the concept of tokens. In this post, weâ€™ll explore what tokens are, why they matter, and how you can work with them in spaCy, one of the most popular NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51f9b2",
   "metadata": {},
   "source": [
    "spaCy makes tokenization simple and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5fdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23a0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokens =======\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = '''Your data text goes here. It can be multiple sentences, e.g.:\n",
    "Apple is looking at buying \"U.K.\" startup for $1 billion!'''\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"\\n======= Tokens =======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cd06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your\n",
      "data\n",
      "text\n",
      "goes\n",
      "here\n",
      ".\n",
      "It\n",
      "can\n",
      "be\n",
      "multiple\n",
      "sentences\n",
      ",\n",
      "e.g.\n",
      ":\n",
      "\n",
      "\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "\"\n",
      "U.K.\n",
      "\"\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5993e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokenization explanation =======\n",
      "Your \t TOKEN\n",
      "data \t TOKEN\n",
      "text \t TOKEN\n",
      "goes \t TOKEN\n",
      "here \t TOKEN\n",
      ". \t SUFFIX\n",
      "It \t TOKEN\n",
      "can \t TOKEN\n",
      "be \t TOKEN\n",
      "multiple \t TOKEN\n",
      "sentences \t TOKEN\n",
      ", \t SUFFIX\n",
      "e.g. \t SPECIAL-1\n",
      ": \t SUFFIX\n",
      "Apple \t TOKEN\n",
      "is \t TOKEN\n",
      "looking \t TOKEN\n",
      "at \t TOKEN\n",
      "buying \t TOKEN\n",
      "\" \t PREFIX\n",
      "U.K. \t TOKEN\n",
      "\" \t SUFFIX\n",
      "startup \t TOKEN\n",
      "for \t TOKEN\n",
      "$ \t PREFIX\n",
      "1 \t TOKEN\n",
      "billion \t TOKEN\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======= Tokenization explanation =======\")\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552529a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokens information =======\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======= Tokens information =======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b9b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Your,    lemmatization: your,    pos: PRON,    is_alpha: True,    is_stopword: True\n",
      "token: data,    lemmatization: data,    pos: NOUN,    is_alpha: True,    is_stopword: False\n",
      "token: text,    lemmatization: text,    pos: NOUN,    is_alpha: True,    is_stopword: False\n",
      "token: goes,    lemmatization: go,    pos: VERB,    is_alpha: True,    is_stopword: False\n",
      "token: here,    lemmatization: here,    pos: ADV,    is_alpha: True,    is_stopword: True\n",
      "token: .,    lemmatization: .,    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: It,    lemmatization: it,    pos: PRON,    is_alpha: True,    is_stopword: True\n",
      "token: can,    lemmatization: can,    pos: AUX,    is_alpha: True,    is_stopword: True\n",
      "token: be,    lemmatization: be,    pos: AUX,    is_alpha: True,    is_stopword: True\n",
      "token: multiple,    lemmatization: multiple,    pos: ADJ,    is_alpha: True,    is_stopword: False\n",
      "token: sentences,    lemmatization: sentence,    pos: NOUN,    is_alpha: True,    is_stopword: False\n",
      "token: ,,    lemmatization: ,,    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: e.g.,    lemmatization: e.g.,    pos: ADV,    is_alpha: False,    is_stopword: False\n",
      "token: :,    lemmatization: :,    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: \n",
      ",    lemmatization: \n",
      ",    pos: SPACE,    is_alpha: False,    is_stopword: False\n",
      "token: Apple,    lemmatization: Apple,    pos: PROPN,    is_alpha: True,    is_stopword: False\n",
      "token: is,    lemmatization: be,    pos: AUX,    is_alpha: True,    is_stopword: True\n",
      "token: looking,    lemmatization: look,    pos: VERB,    is_alpha: True,    is_stopword: False\n",
      "token: at,    lemmatization: at,    pos: ADP,    is_alpha: True,    is_stopword: True\n",
      "token: buying,    lemmatization: buy,    pos: VERB,    is_alpha: True,    is_stopword: False\n",
      "token: \",    lemmatization: \",    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: U.K.,    lemmatization: U.K.,    pos: PROPN,    is_alpha: False,    is_stopword: False\n",
      "token: \",    lemmatization: \",    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: startup,    lemmatization: startup,    pos: NOUN,    is_alpha: True,    is_stopword: False\n",
      "token: for,    lemmatization: for,    pos: ADP,    is_alpha: True,    is_stopword: True\n",
      "token: $,    lemmatization: $,    pos: SYM,    is_alpha: False,    is_stopword: False\n",
      "token: 1,    lemmatization: 1,    pos: NUM,    is_alpha: False,    is_stopword: False\n",
      "token: billion,    lemmatization: billion,    pos: NUM,    is_alpha: True,    is_stopword: False\n",
      "token: !,    lemmatization: !,    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "\n",
      "======= Customization =======\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"\"\"token: {token.text},\\\n",
    "    lemmatization: {token.lemma_},\\\n",
    "    pos: {token.pos_},\\\n",
    "    is_alpha: {token.is_alpha},\\\n",
    "    is_stopword: {token.is_stop}\"\"\")\n",
    "\n",
    "print(\"\\n======= Customization =======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762b7c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gon', 'na', 'to', 'the', 'Beach']\n"
     ]
    }
   ],
   "source": [
    "# customization\n",
    "text = (\"Gonna to the Beach\")\n",
    "doc = nlp(text)\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a218734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special case rule\n",
    "special_case = [{ORTH: \"Gon\"}, {ORTH: \"na\"}]\n",
    "nlp.tokenizer.add_special_case(\"Gonna\", special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21041b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gon', 'na', 'to', 'the', 'Beach']\n"
     ]
    }
   ],
   "source": [
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"Gonna to the Beach\")])  #['Gon', 'na', 'to', 'the', 'Beach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['....', 'Gon', 'na', 'now', '!', '!', '!', '!', 'there', '*', '*', '<', '/', '>']\n"
     ]
    }
   ],
   "source": [
    "# The special case rules have precedence over the punctuation splitting\n",
    "doc = nlp(\"....Gonna now !!!! there ** </>\")    # phrase to tokenize\n",
    "print([w.text for w in doc])  #['....', 'Gon', 'na', 'now', '!', '!', '!', '!', 'there', '*', '*', '<', '/', '>']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ad945a",
   "metadata": {},
   "source": [
    "The Unigram algorithm \n",
    "\n",
    "is used in combination with SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\n",
    "\n",
    "SentencePiece addresses the fact that not all languages use spaces to separate words. Instead, SentencePiece treats the input as a raw input stream which includes the space in the set of characters to use. Then it can use the Unigram algorithm to construct the appropriate vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32a489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c5198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/home/silver/NLP_Basics/Tokenazation/phx.txt'\n",
    "model = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448ec9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command:     --model_type=unigram    --input=/home/silver/NLP_Basics/Tokenazation/phx.txt    --model_prefix=models/uni    --vocab_size=500\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "  input_format: \n",
      "  model_prefix: models/uni\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /home/silver/NLP_Basics/Tokenazation/phx.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9349 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1493352\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9587% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999588\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 9349 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=788000\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 36622 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9349\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 27121\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 27121 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13896 obj=11.0189 num_tokens=56041 num_tokens/piece=4.03289\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11804 obj=8.89289 num_tokens=56300 num_tokens/piece=4.76957\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8851 obj=8.8919 num_tokens=59749 num_tokens/piece=6.75054\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8842 obj=8.86296 num_tokens=59744 num_tokens/piece=6.75684\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6631 obj=9.02726 num_tokens=65497 num_tokens/piece=9.87739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6631 obj=8.98642 num_tokens=65496 num_tokens/piece=9.87724\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4973 obj=9.22656 num_tokens=72502 num_tokens/piece=14.5791\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4973 obj=9.17941 num_tokens=72487 num_tokens/piece=14.5761\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3729 obj=9.4683 num_tokens=79934 num_tokens/piece=21.4358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3729 obj=9.41575 num_tokens=79932 num_tokens/piece=21.4352\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2796 obj=9.76445 num_tokens=87660 num_tokens/piece=31.3519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2796 obj=9.7051 num_tokens=87662 num_tokens/piece=31.3526\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2097 obj=10.1023 num_tokens=95052 num_tokens/piece=45.3276\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2097 obj=10.0327 num_tokens=95054 num_tokens/piece=45.3286\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1572 obj=10.4834 num_tokens=102201 num_tokens/piece=65.0134\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1572 obj=10.4085 num_tokens=102203 num_tokens/piece=65.0146\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1179 obj=10.8942 num_tokens=109388 num_tokens/piece=92.7803\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1179 obj=10.8092 num_tokens=109392 num_tokens/piece=92.7837\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=884 obj=11.3646 num_tokens=116774 num_tokens/piece=132.097\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=884 obj=11.2637 num_tokens=116774 num_tokens/piece=132.097\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=663 obj=11.8339 num_tokens=123595 num_tokens/piece=186.418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=663 obj=11.7241 num_tokens=123595 num_tokens/piece=186.418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=550 obj=12.103 num_tokens=127629 num_tokens/piece=232.053\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=550 obj=12.0367 num_tokens=127644 num_tokens/piece=232.08\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/uni.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/uni.vocab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=unigram\\\n",
    "    --input={data}\\\n",
    "    --model_prefix={model}/uni\\\n",
    "    --vocab_size=500''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f00b15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(model, 'uni.model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fdd42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "[169, 48, 90, 158, 18, 131, 228]\n"
     ]
    }
   ],
   "source": [
    "input_string = \"This is a test\"\n",
    "# encode: text => id\n",
    "# by default a space is added at the start of the input sentence\n",
    "print(sp.encode_as_pieces(input_string))    # ['▁This', '▁is', '▁a', '▁t', 'est']\n",
    "print(sp.encode_as_ids(input_string))       # [371, 77, 13, 101, 181]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fecd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁This is a test\n",
      "take d'.' sp\n",
      "vocab size: 500\n",
      "id 371 to piece: ▁take\n",
      "Piece ▁This to id: 0\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))    # This is a test\n",
    "print(sp.decode_ids([371, 77, 13, 101, 181]))                     # This is a test\n",
    "\n",
    "# returns vocab size\n",
    "print(f\"vocab size: {sp.get_piece_size()}\")\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(f\"id 371 to piece: {sp.id_to_piece(371)}\")\n",
    "print(f\"Piece ▁This to id: {sp.piece_to_id('▁This')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
